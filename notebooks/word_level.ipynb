{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.layers import Embedding\r\n",
    "from keras.layers import GRU\r\n",
    "from keras.layers import Activation\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import io\r\n",
    "import json\r\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Loading the training data\r\n",
    "path = '../data/manele-merged.json'\r\n",
    "text = []\r\n",
    "with io.open(path, encoding=\"utf-8\") as f:\r\n",
    "    file = json.load(f)\r\n",
    "    for manea in file:\r\n",
    "        for lyric in manea['lyrics']:\r\n",
    "            text.append(lyric)\r\n",
    "text = ''.join(text)\r\n",
    "\r\n",
    "# Cleaning the text\r\n",
    "text = text.lower()\r\n",
    "to_replace = list('!\"$&()*+/:;<=>@[]^_~{}#%\\\\|–…\\ufeff\\xa0§«»')\r\n",
    "to_replace.append(\"'\")\r\n",
    "to_replace.append(\"refren\")\r\n",
    "to_replace.append(\"ref\")\r\n",
    "to_replace.append(\"florin salam\")\r\n",
    "to_replace.append(\"salam\")\r\n",
    "to_replace.append(\"bis\")\r\n",
    "to_replace.append(\"augustin\")\r\n",
    "to_replace.append(\"nicolae guta\")\r\n",
    "to_replace.append(\"nicoleta guta\")\r\n",
    "to_replace.append(\"guta\")\r\n",
    "to_replace.append(\"costel biju\")\r\n",
    "to_replace.append(\"liviu pustiu\")\r\n",
    "to_replace.append(\"dani mocanu\")\r\n",
    "to_replace.append(\"vali vijelie\")\r\n",
    "to_replace.append(\"solo\")\r\n",
    "to_replace.append(\"x2\")\r\n",
    "to_replace.append(\"2x\")\r\n",
    "\r\n",
    "for word in to_replace:\r\n",
    "    text = text.replace(word, '')\r\n",
    "\r\n",
    "text = re.sub('â|ă|а', 'a', text)\r\n",
    "text = re.sub('í|î|ï|і|ἰ', 'i', text)\r\n",
    "text = re.sub('ş|ș|ѕ', 's', text)\r\n",
    "text = re.sub('ţ', 't', text)\r\n",
    "text = re.sub('ν', 'v', text)\r\n",
    "text = re.sub('в', 'b', text)\r\n",
    "text = re.sub('е', 'e', text)\r\n",
    "text = re.sub('к', 'k', text)\r\n",
    "text = re.sub('м', 'm', text)\r\n",
    "text = re.sub('н', 'h', text)\r\n",
    "text = re.sub('о', 'o', text)\r\n",
    "text = re.sub('р', 'p', text)\r\n",
    "text = re.sub('с', 'c', text)\r\n",
    "text = re.sub('т', 't', text)\r\n",
    "text = re.sub('у', 'y', text)\r\n",
    "text = re.sub('х', 'x', text)\r\n",
    "text = re.sub('ј', 'j', text)\r\n",
    "\r\n",
    "\r\n",
    "text = re.sub(r'\\d\\.', '', text)\r\n",
    "text = re.sub(r'st?rofa \\d*', '', text)\r\n",
    "text = re.sub(r'-{2,}', '', text)\r\n",
    "text = re.sub(r'sh', 's', text)\r\n",
    "text = re.sub(r'\\.{4,}', '...', text)\r\n",
    "text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\r\n",
    "print(\"Corpus length:\", len(text))\r\n",
    "\r\n",
    "chars = sorted(list(set(text)))\r\n",
    "print(chars)\r\n",
    "print(\"Total chars:\", len(chars))\r\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\r\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 1577240\n",
      "['\\n', ' ', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Total chars: 42\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "text = text.replace('\\n', ' \\n ')\r\n",
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_freq = {}\r\n",
    "for word in text_in_words:\r\n",
    "    word_freq{word} = word_freq.get(word, 0) + 1\r\n",
    "MIN_WORD_FREQUENCY = 450\r\n",
    "ignored_words = set()\r\n",
    "for k, v in word_freq.items():\r\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\r\n",
    "        ignored_words.add(k)\r\n",
    "words = set(text_in_words)\r\n",
    "print('Unique words before ignoring:', len(words))\r\n",
    "# print('Ignoring words with frequency')\r\n",
    "words = sorted(set(words) - ignored_words)\r\n",
    "print('Unique words after ignoring:', len(words))\r\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\r\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "\r\n",
    "# cut the text in semi-redundant sequences of maxlen characters\r\n",
    "maxlen = 25\r\n",
    "step = 3\r\n",
    "sentences = []\r\n",
    "next_chars = []\r\n",
    "for i in range(0, len(text) - maxlen, step):\r\n",
    "    sentences.append(text[i : i + maxlen])\r\n",
    "    next_chars.append(text[i + maxlen])\r\n",
    "print(\"Number of sequences:\", len(sentences))\r\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\r\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\r\n",
    "for i, sentence in enumerate(sentences):\r\n",
    "    for t, char in enumerate(sentence):\r\n",
    "        x[i, t, char_indices[char]] = 1\r\n",
    "    y[i, char_indices[next_chars[i]]] = 1\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of sequences: 609665\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = Sequential()\r\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(LSTM(256))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2 = Sequential()\r\n",
    "model2.add(Embedding(len(words), 64))\r\n",
    "model2.add(Dropout(0.2))\r\n",
    "model2.add(GRU(64))\r\n",
    "model2.add(Dropout(0.2))\r\n",
    "model2.add(Dense(len(words)))\r\n",
    "model2.add(Activation('softmax'))\r\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def sample(preds, temperature=1.0):\r\n",
    "    # helper function to sample an index from a probability array\r\n",
    "    preds = np.asarray(preds).astype(\"float64\")\r\n",
    "    preds = np.log(preds) / temperature\r\n",
    "    exp_preds = np.exp(preds)\r\n",
    "    preds = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, preds, 1)\r\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "epochs = 3\r\n",
    "batch_size = 128 \r\n",
    "\r\n",
    "model.fit(x, y, batch_size=batch_size, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "4108/4108 [==============================] - 116s 27ms/step - loss: 1.9586\n",
      "Epoch 2/3\n",
      "4108/4108 [==============================] - 102s 25ms/step - loss: 1.5533\n",
      "Epoch 3/3\n",
      "4108/4108 [==============================] - 103s 25ms/step - loss: 1.4338\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd8ce117b8>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "generated_length = 300\r\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\r\n",
    "for diversity in [0.2, 0.5]:\r\n",
    "    print(\"...Diversity:\", diversity)\r\n",
    "\r\n",
    "    generated = \"\"\r\n",
    "    # original_sentence = text[start_index : start_index + maxlen]\r\n",
    "    original_sentence = \"iubire\"\r\n",
    "    sentence = original_sentence\r\n",
    "    print('...Generating with seed:\\n \"' + sentence + '\"')\r\n",
    "\r\n",
    "    for i in range(generated_length):\r\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\r\n",
    "        for t, char in enumerate(sentence):\r\n",
    "            x_pred[0, t, char_indices[char]] = 1.0\r\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\r\n",
    "        next_index = sample(preds, diversity)\r\n",
    "        next_char = indices_char[next_index]\r\n",
    "        sentence = sentence[1:] + next_char\r\n",
    "        generated += next_char\r\n",
    "\r\n",
    "    print(\"...Generated:\\n\", original_sentence + generated)\r\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...Diversity: 0.2\n",
      "...Generating with seed:\n",
      " \"iubire\"\n",
      "...Generated:\n",
      " iubire      oo    yyy966q9758599984y666965y6q96959999888699666w9996796969650yq996166969959899986999999699999968969666995996w9866699q55y9994w9866998q9999w9999699669q965965y895q9994y666967q5y555y69y6555y5yy9956969996999999699696995q9999y969996969675y5q55y5yq89696975y5qq595y58699696689996999969999999844y6696\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed:\n",
      " \"iubire\"\n",
      "...Generated:\n",
      " iubire s y g hyyhyy65q555yy58y5q6858y5888969986979975w99w66qq95559yy695q594yy966699w9996671886448hy81hgy999hdqy669q55hy1y.yy668357y8q989q86969564yq696755hyyyy.9q666865118y864w7996695495y689667936y9568996hyw81ww90997w64w689645.w80yw0668w86999699656y97q955y599yq64966w9q5558y9619666406161wy5969q641648144y798\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model.save(\"../models/model-15-1.05.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Save weights in hdf5 format\r\n",
    "filepath=\"model-{epoch:02d}-{loss:.4f}.h5\"\r\n",
    "checkpoint = ModelCheckpoint(filepath + \".hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('dl': conda)"
  },
  "interpreter": {
   "hash": "fbae5d8352c404731591c92cb8cc38e6f297e7599cbd2fb7fa464e01d643be22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}