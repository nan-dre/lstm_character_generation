{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding, GRU\r\n",
    "import numpy as np\r\n",
    "import sys\r\n",
    "import io\r\n",
    "import os\r\n",
    "import re\r\n",
    "import json\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Parameters: change to experiment different configurations\r\n",
    "SEQUENCE_LEN = 6\r\n",
    "MIN_WORD_FREQUENCY = 50\r\n",
    "STEP = 1\r\n",
    "BATCH_SIZE = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path = '../data/manele-merged.json'\r\n",
    "text = []\r\n",
    "with io.open(path, encoding=\"utf-8\") as f:\r\n",
    "    file = json.load(f)\r\n",
    "    for manea in file:\r\n",
    "        for lyric in manea['lyrics']:\r\n",
    "            text.append(lyric)\r\n",
    "text = ''.join(text)\r\n",
    "\r\n",
    "# Cleaning the text\r\n",
    "text = text.lower()\r\n",
    "to_replace = list('!\"$&()*+/:;<=>@[]^_~{}#%\\\\|–…\\ufeff\\xa0§«»')\r\n",
    "to_replace.append(\"'\")\r\n",
    "to_replace.append(\"refren\")\r\n",
    "to_replace.append(\"ref\")\r\n",
    "to_replace.append(\"florin salam\")\r\n",
    "to_replace.append(\"salam\")\r\n",
    "to_replace.append(\"bis\")\r\n",
    "to_replace.append(\"augustin\")\r\n",
    "to_replace.append(\"nicolae guta\")\r\n",
    "to_replace.append(\"nicoleta guta\")\r\n",
    "to_replace.append(\"guta\")\r\n",
    "to_replace.append(\"costel biju\")\r\n",
    "to_replace.append(\"liviu pustiu\")\r\n",
    "to_replace.append(\"dani mocanu\")\r\n",
    "to_replace.append(\"vali vijelie\")\r\n",
    "to_replace.append(\"solo\")\r\n",
    "to_replace.append(\"x2\")\r\n",
    "to_replace.append(\"2x\")\r\n",
    "to_replace.append(\"x4\")\r\n",
    "\r\n",
    "for word in to_replace:\r\n",
    "    text = text.replace(word, '')\r\n",
    "\r\n",
    "text = re.sub('â|ă|а', 'a', text)\r\n",
    "text = re.sub('í|î|ï|і|ἰ', 'i', text)\r\n",
    "text = re.sub('ş|ș|ѕ', 's', text)\r\n",
    "text = re.sub('ţ', 't', text)\r\n",
    "text = re.sub('ν', 'v', text)\r\n",
    "text = re.sub('в', 'b', text)\r\n",
    "text = re.sub('е', 'e', text)\r\n",
    "text = re.sub('к', 'k', text)\r\n",
    "text = re.sub('м', 'm', text)\r\n",
    "text = re.sub('н', 'h', text)\r\n",
    "text = re.sub('о', 'o', text)\r\n",
    "text = re.sub('р', 'p', text)\r\n",
    "text = re.sub('с', 'c', text)\r\n",
    "text = re.sub('т', 't', text)\r\n",
    "text = re.sub('у', 'y', text)\r\n",
    "text = re.sub('х', 'x', text)\r\n",
    "text = re.sub('ј', 'j', text)\r\n",
    "text = re.sub('k', 'ca', text)\r\n",
    "\r\n",
    "\r\n",
    "text = re.sub(r'\\d\\.', '', text)\r\n",
    "text = re.sub(r'st?rofa \\d*', '', text)\r\n",
    "text = re.sub(r'-{2,}', '', text)\r\n",
    "text = re.sub(r'sh', 's', text)\r\n",
    "text = re.sub(r'\\.{4,}', '...', text)\r\n",
    "text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\r\n",
    "text = text.replace('\\n', ' \\n ')\r\n",
    "print(\"Corpus length:\", len(text))\r\n",
    "\r\n",
    "chars = sorted(list(set(text)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 1704405\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\r\n",
    "print('Corpus length in words:', len(text_in_words))\r\n",
    "\r\n",
    "# Calculate word frequency\r\n",
    "word_freq = {}\r\n",
    "for word in text_in_words:\r\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\r\n",
    "\r\n",
    "ignored_words = set()\r\n",
    "for k, v in word_freq.items():\r\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\r\n",
    "        ignored_words.add(k)\r\n",
    "\r\n",
    "words = set(text_in_words)\r\n",
    "print('Unique words before ignoring:', len(words))\r\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\r\n",
    "words = sorted(set(words) - ignored_words)\r\n",
    "print('Unique words after ignoring:', len(words))\r\n",
    "\r\n",
    "with open(\"../data/vocabulary.txt\", \"w\") as f:\r\n",
    "    for w in words:\r\n",
    "        f.write(\"%s\\n\" % w)\r\n",
    "print(words)\r\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\r\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))\r\n",
    "\r\n",
    "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\r\n",
    "sentences_original = []\r\n",
    "next_words_original = []\r\n",
    "ignored = 0\r\n",
    "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\r\n",
    "    # Only add the sequences where no word is in ignored_words\r\n",
    "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\r\n",
    "        sentences_original.append(text_in_words[i: i + SEQUENCE_LEN])\r\n",
    "        next_words_original.append(text_in_words[i + SEQUENCE_LEN])\r\n",
    "    else:\r\n",
    "        ignored = ignored + 1\r\n",
    "print('Ignored sequences:', ignored)\r\n",
    "print('Remaining sequences:', len(sentences_original))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length in words: 375411\n",
      "Unique words before ignoring: 15289\n",
      "Ignoring words with frequency < 150\n",
      "Unique words after ignoring: 264\n",
      "['\\n', '...', '2', 'a', 'acum', 'ai', 'al', 'altcineva', 'am', 'ar', 'are', 'as', 'asa', 'asta', 'astazi', 'atat', 'atunci', 'au', 'avut', 'azi', 'bani', 'banii', 'beau', 'bine', 'buna', 'ca', 'cand', 'care', 'casa', 'cat', 'cate', 'ce', 'cea', 'cel', 'cer', 'ceva', 'chiar', 'cine', 'cred', 'cu', 'cum', 'da', 'da,', 'daca', 'dai', 'dar', 'dat', 'dau', 'de', 'decat', 'deloc', 'din', 'doamne', 'doar', 'dor', 'doua', 'dragoste', 'dragostea', 'duc', 'dumnezeu', 'dupa', 'dusmani', 'dusmanii', 'e', 'ea', 'ei', 'el', 'este', 'esti', 'eu', 'fac', 'face', 'faci', 'facut', 'fara', 'fata', 'fel', 'femeie', 'fericit', 'fi', 'fie', 'fii', 'fiu', 'foc', 'fost', 'frumoasa', 'frumos', 'greu', 'hai', 'ia', 'iar', 'iau', 'ii', 'il', 'imi', 'in', 'inima', 'inimioara', 'iti', 'iubesc', 'iubesti', 'iubi', 'iubire', 'iubirea', 'iubit', 'jur', 'la', 'lacrimi', 'lai', 'langa', 'las', 'lasa', 'lasat', 'le', 'loc', 'luat', 'lume', 'lumea', 'm-a', 'm-ai', 'm-am', 'ma', 'macar', 'mai', 'maine', 'mama', 'mana', 'mare', 'mea', 'mea,', 'mea...', 'mei', 'mele', 'mereu', 'meu', 'mi', 'mi-a', 'mi-ai', 'mi-e', 'mie', 'mine', 'mine,', 'mor', 'mult', 'multe', 'n-ai', 'n-am', 'na', 'ne', 'ne-am', 'nici', 'niciodata', 'nimeni', 'nimic', 'noapte', 'noaptea', 'noi', 'noroc', 'nu', 'nu-i', 'nu-mi', 'numai', 'o', 'oare', 'ochii', 'of', 'om', 'ori', 'orice', 'pana', 'parca', 'pare', 'pe', 'pentru', 'peste', 'pierdut', 'place', 'plang', 'plange', 'plecat', 'pleci', 'poate', 'pot', 'poti', 'prea', 'prin', 'pt', 'pun', 'pus', 'putea', 'putere', 'rau', 'rea', 'rog', 's-a', 's-o', 'sa', 'sa-mi', 'sa-ti', 'sau', 'se', 'si', 'simt', 'singur', 'spui', 'spun', 'spune', 'spune-mi', 'spus', 'sta', 'stai', 'stau', 'stie', 'stii', 'stiu', 'suflet', 'sufletul', 'sunt', 'ta', 'tai', 'tare', 'tata', 'tau', 'te', 'te-ai', 'te-am', 'te-as', 'ti-am', 'tin', 'tine', 'tine,', 'toata', 'toate', 'tot', 'toti', 'traiesc', 'trecut', 'tu', 'uit', 'un', 'una', 'unde', 'urma', 'usor', 'va', 'vad', 'valoare', 'vazut', 'vei', 'vezi', 'viata', 'viatza', 'vin', 'vina', 'vine', 'voi', 'vrea', 'vreau', 'vrei', 'vrut', 'x', 'zi', 'zile', 'zilele']\n",
      "Ignored sequences: 337389\n",
      "Remaining sequences: 38016\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# shuffle at unison\r\n",
    "print('Shuffling sentences')\r\n",
    "percentage_test = 2\r\n",
    "\r\n",
    "tmp_sentences = []\r\n",
    "tmp_next_word = []\r\n",
    "for i in np.random.permutation(len(sentences_original)):\r\n",
    "    tmp_sentences.append(sentences_original[i])\r\n",
    "    tmp_next_word.append(next_words_original[i])\r\n",
    "\r\n",
    "cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\r\n",
    "sentences, sentences_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\r\n",
    "next_words, next_words_test= tmp_next_word[:cut_index], tmp_next_word[cut_index:]\r\n",
    "\r\n",
    "print(\"Size of training set = %d\" % len(sentences))\r\n",
    "print(\"Size of test set = %d\" % len(sentences_test))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 112929\n",
      "Size of test set = 2305\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Data generator for fit and evaluate\r\n",
    "def generator(sentence_list, next_word_list, batch_size):\r\n",
    "    index = 0\r\n",
    "    while True:\r\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\r\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\r\n",
    "        for i in range(batch_size):\r\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\r\n",
    "                x[i, t] = word_indices[w]\r\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\r\n",
    "            index = index + 1\r\n",
    "        yield x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Embedding(input_dim=len(words), output_dim=1024))\r\n",
    "model.add(Bidirectional(LSTM(128)))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(Dense(len(words)))\r\n",
    "model.add(Activation('softmax'))\r\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def sample(preds, temperature=1.0):\r\n",
    "    # helper function to sample an index from a probability array\r\n",
    "    preds = np.asarray(preds).astype('float64')\r\n",
    "    preds = np.log(preds) / temperature\r\n",
    "    exp_preds = np.exp(preds)\r\n",
    "    preds = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, preds, 1)\r\n",
    "    return np.argmax(probas)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def on_epoch_end(epoch, logs):\r\n",
    "    # Function invoked at end of each epoch. Prints generated text.\r\n",
    "    examples_file = open(\"../data/examples.txt\", \"a\")\r\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\r\n",
    "\r\n",
    "    # Randomly pick a seed sequence\r\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\r\n",
    "    seed = (sentences+sentences_test)[seed_index]\r\n",
    "\r\n",
    "    for diversity in [0.3, 0.5]:\r\n",
    "        sentence = seed\r\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\r\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\r\n",
    "        examples_file.write(' '.join(sentence))\r\n",
    "\r\n",
    "        for i in range(50):\r\n",
    "            x_pred = np.zeros((1, SEQUENCE_LEN))\r\n",
    "            for t, word in enumerate(sentence):\r\n",
    "                x_pred[0, t] = word_indices[word]\r\n",
    "\r\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\r\n",
    "            next_index = sample(preds, diversity)\r\n",
    "            next_word = indices_word[next_index]\r\n",
    "\r\n",
    "            sentence = sentence[1:]\r\n",
    "            sentence.append(next_word)\r\n",
    "\r\n",
    "            examples_file.write(\" \"+next_word)\r\n",
    "        examples_file.write('\\n')\r\n",
    "    examples_file.write('='*80 + '\\n')\r\n",
    "    examples_file.flush()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\r\n",
    "callbacks_list = [print_callback]\r\n",
    "model.fit(generator(sentences, next_words, BATCH_SIZE),\r\n",
    "                    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\r\n",
    "                    epochs=15,\r\n",
    "                    callbacks=callbacks_list,\r\n",
    "                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\r\n",
    "                    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "3530/3530 [==============================] - 60s 15ms/step - loss: 3.8568 - accuracy: 0.2698 - val_loss: 3.1049 - val_accuracy: 0.3639\n",
      "Epoch 2/15\n",
      "3530/3530 [==============================] - 47s 13ms/step - loss: 2.8001 - accuracy: 0.4110 - val_loss: 2.5771 - val_accuracy: 0.4700\n",
      "Epoch 3/15\n",
      "3530/3530 [==============================] - 51s 14ms/step - loss: 2.2446 - accuracy: 0.5134 - val_loss: 2.2674 - val_accuracy: 0.5253\n",
      "Epoch 4/15\n",
      "3530/3530 [==============================] - 50s 14ms/step - loss: 1.8647 - accuracy: 0.5899 - val_loss: 2.0788 - val_accuracy: 0.5762\n",
      "Epoch 5/15\n",
      "1830/3530 [==============>...............] - ETA: 22s - loss: 1.6591 - accuracy: 0.6258"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-12555a5d2a85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n\u001b[0m",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"../models/saved_model_2.h5\", save_format=\"h5\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('dl': conda)"
  },
  "interpreter": {
   "hash": "fbae5d8352c404731591c92cb8cc38e6f297e7599cbd2fb7fa464e01d643be22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}