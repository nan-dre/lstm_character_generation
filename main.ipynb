{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import json\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#path = keras.utils.get_file(\n",
    "#     \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "# )\n",
    "path = './manele.json'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text = []\n",
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    file = json.load(f)\n",
    "    for manea in file:\n",
    "        for lyric in manea['lyrics']:\n",
    "            text.append(lyric)\n",
    "text = ''.join(text)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text = text.lower()\n",
    "to_replace = list('!\"$&()*+/:;<=>@[]^_~{}')\n",
    "to_replace.append(\"refren\")\n",
    "to_replace.append(\"x2\")\n",
    "to_replace.append(\"florin salam\")\n",
    "for word in to_replace:\n",
    "    text = text.replace(word, '')\n",
    "print(\"Corpus length:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "print(\"Total chars:\", len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 400522\n",
      "['\\n', ' ', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Total chars: 42\n",
      "Number of sequences: 133488\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(chars))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(chars), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "#optimizer='adam'\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "tensor_board = TensorBoard('./logs/character_generation')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "epochs = 10\n",
    "batch_size = 64 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: %d\\n\" % epoch)\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "\n",
      "2086/2086 [==============================] - 18s 6ms/step - loss: 1.8971\n",
      "Epoch: 1\n",
      "\n",
      "2086/2086 [==============================] - 14s 6ms/step - loss: 1.5521\n",
      "Epoch: 2\n",
      "\n",
      "2086/2086 [==============================] - 14s 7ms/step - loss: 1.4507\n",
      "Epoch: 3\n",
      "\n",
      "2086/2086 [==============================] - 13s 6ms/step - loss: 1.3950\n",
      "Epoch: 4\n",
      "\n",
      "2086/2086 [==============================] - 14s 7ms/step - loss: 1.3683\n",
      "Epoch: 5\n",
      "\n",
      "2086/2086 [==============================] - 14s 6ms/step - loss: 1.3355\n",
      "Epoch: 6\n",
      "\n",
      "2086/2086 [==============================] - 13s 6ms/step - loss: 1.3147\n",
      "Epoch: 7\n",
      "\n",
      "2086/2086 [==============================] - 13s 6ms/step - loss: 1.2904\n",
      "Epoch: 8\n",
      "\n",
      "2086/2086 [==============================] - 13s 6ms/step - loss: 1.2631\n",
      "Epoch: 9\n",
      "\n",
      "2086/2086 [==============================] - 13s 6ms/step - loss: 1.2697\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "generated_length = 300\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5]:\n",
    "    print(\"...Diversity:\", diversity)\n",
    "\n",
    "    generated = \"\"\n",
    "    original_sentence = text[start_index : start_index + maxlen]\n",
    "    sentence = original_sentence\n",
    "    print('...Generating with seed:\\n \"' + sentence + '\"')\n",
    "\n",
    "    for i in range(generated_length):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.0\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "\n",
    "    print(\"...Generated:\\n\", original_sentence + generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...Diversity: 0.2\n",
      "...Generating with seed:\n",
      " \"ca la inceput\n",
      "nu mi-am dat seama ce am avut\n",
      "pana cand eu nu \"\n",
      "...Generated:\n",
      " ca la inceput\n",
      "nu mi-am dat seama ce am avut\n",
      "pana cand eu nu ma iubesti si nu mai pot ierta\n",
      "\n",
      "am sa te vad cu mine \n",
      "cand iti pare de mine\n",
      "sa te vad cum te pot iubire\n",
      "ca am parte si te port in mine\n",
      "si te vad cu mine ca tine ma iubesti \n",
      "sa te iubesti de mine\n",
      "\n",
      "am s\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed:\n",
      " \"ca la inceput\n",
      "nu mi-am dat seama ce am avut\n",
      "pana cand eu nu \"\n",
      "...Generated:\n",
      " ca la inceput\n",
      "nu mi-am dat seama ce am avut\n",
      "pana cand eu nu ma iubesti pentru tine\n",
      "in fata mea ma iubesti de-ar vrea sa te vad si eu ca a mea de tine \n",
      "am norocul meu\n",
      "dar nu vreau nu vreau sa mai pot ierta\n",
      "\n",
      "pentru tine nu vreau sa te mai iubesc\n",
      "cand vai de mine\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('deeplearning': conda)"
  },
  "interpreter": {
   "hash": "eddcb13c0afbe40eff33f8afdc713c878396e4478ab766ce2943b14c50b9ae00"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}