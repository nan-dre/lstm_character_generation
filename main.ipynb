{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import json\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#path = keras.utils.get_file(\n",
    "#     \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "# )\n",
    "path = './manele.json'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text = []\n",
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    file = json.load(f)\n",
    "    for manea in file:\n",
    "        for lyric in manea['lyrics']:\n",
    "            text.append(lyric)\n",
    "text = ''.join(text)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text = text.lower()\n",
    "to_replace = list('!\"$&()*+/:;<=>@[]^_~{}')\n",
    "to_replace.append(\"refren\")\n",
    "to_replace.append(\"x2\")\n",
    "to_replace.append(\"florin salam\")\n",
    "for word in to_replace:\n",
    "    text = text.replace(word, '')\n",
    "print(\"Corpus length:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "print(\"Total chars:\", len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 60\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 401854\n",
      "['\\n', ' ', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Total chars: 42\n",
      "Number of sequences: 133932\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(chars))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(chars), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "#optimizer='adam'\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "tensor_board = TensorBoard('./logs/character_generation')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: %d\\n\" % epoch)\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "\n",
      "1047/1047 [==============================] - 13s 10ms/step - loss: 1.9620\n",
      "Epoch: 1\n",
      "\n",
      "1047/1047 [==============================] - 11s 11ms/step - loss: 1.5602\n",
      "Epoch: 2\n",
      "\n",
      "1047/1047 [==============================] - 12s 11ms/step - loss: 1.4312\n",
      "Epoch: 3\n",
      "\n",
      "1047/1047 [==============================] - 12s 11ms/step - loss: 1.3604\n",
      "Epoch: 4\n",
      "\n",
      "1047/1047 [==============================] - 11s 10ms/step - loss: 1.3117\n",
      "Epoch: 5\n",
      "\n",
      "1047/1047 [==============================] - 11s 10ms/step - loss: 1.2771\n",
      "Epoch: 6\n",
      "\n",
      "1047/1047 [==============================] - 10s 10ms/step - loss: 1.2496\n",
      "Epoch: 7\n",
      "\n",
      "1047/1047 [==============================] - 10s 10ms/step - loss: 1.2253\n",
      "Epoch: 8\n",
      "\n",
      "1047/1047 [==============================] - 10s 10ms/step - loss: 1.2097\n",
      "Epoch: 9\n",
      "\n",
      "1047/1047 [==============================] - 10s 10ms/step - loss: 1.1933\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "generated_length = 200\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5]:\n",
    "    print(\"...Diversity:\", diversity)\n",
    "\n",
    "    generated = \"\"\n",
    "    original_sentence = text[start_index : start_index + maxlen]\n",
    "    sentence = original_sentence\n",
    "    print('...Generating with seed:\\n \"' + sentence + '\"')\n",
    "\n",
    "    for i in range(generated_length):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.0\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "\n",
    "    print(\"...Generated:\\n\", original_sentence + generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...Diversity: 0.2\n",
      "...Generating with seed:\n",
      " \"nu vrei ce nu-ti convine\n",
      "stiu ca ma iubesti pe mine. \n",
      "\n",
      "stiu \"\n",
      "...Generated:\n",
      " nu vrei ce nu-ti convine\n",
      "stiu ca ma iubesti pe mine. \n",
      "\n",
      "stiu ca sa te mai pot azi inima mea. \n",
      "\n",
      "florin salam\n",
      "e doamne sunt si sa mai placesc sa nu te las\n",
      "as vrea sa te iubesc\n",
      "cum sa te iubesc\n",
      "dar nu ma las sa fii numai pt mine\n",
      "\n",
      "ce as vrea sa ma dau\n",
      "si nu mai pot\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed:\n",
      " \"nu vrei ce nu-ti convine\n",
      "stiu ca ma iubesti pe mine. \n",
      "\n",
      "stiu \"\n",
      "...Generated:\n",
      " nu vrei ce nu-ti convine\n",
      "stiu ca ma iubesti pe mine. \n",
      "\n",
      "stiu ca nu ma las la dusmanii mei nu te las\n",
      "si sa ma dau\n",
      "doamne si numai sunosit de ceasa mele doamne este privat\n",
      "si imi plac nimenea sa nu mai las la locul tau si am plans din tine nu ma las cu tine as vr\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('deeplearning': conda)"
  },
  "interpreter": {
   "hash": "eddcb13c0afbe40eff33f8afdc713c878396e4478ab766ce2943b14c50b9ae00"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}