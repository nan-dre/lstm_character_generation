{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path = keras.utils.get_file(\n",
    "    \"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
    "print(\"Corpus length:\", len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"Total chars:\", len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Corpus length: 600893\n",
      "Total chars: 56\n",
      "Number of sequences: 200285\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(chars))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(chars), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "#optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "optimizer='adam'\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "tensor_board = TensorBoard('./logs/character_generation')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: %d\\n\" % epoch)\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "\n",
    "print()\n",
    "print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print(\"...Diversity:\", diversity)\n",
    "\n",
    "    generated = \"\"\n",
    "    sentence = text[start_index : start_index + maxlen]\n",
    "    print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.0\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "\n",
    "    print(\"...Generated: \", generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "\n",
      "3130/3130 [==============================] - 19s 5ms/step - loss: 2.3841\n",
      "Epoch: 1\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 2.0325\n",
      "Epoch: 2\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.8861\n",
      "Epoch: 3\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.7874\n",
      "Epoch: 4\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.7151\n",
      "\n",
      "Generating text after epoch: 4\n",
      "...Diversity: 0.2\n",
      "...Generating with seed: \"em or world-pain in that which he finds \"\n",
      "...Generated:  of the some of the saint and the string the strong to the case of the supprisent and as a moral the still of the superses the string the sainity of the consentions of the consention of the expersed the sense of the supprisent of the world the restined to the consention of the saining the still of the was the sained the consiction of the will of the still of the same the sainity of the present of t\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed: \"em or world-pain in that which he finds \"\n",
      "...Generated:  for the sinday to consinded more in strong and with with where to it is and and perspurions with ease the wishers of his of sherual himself desprciest of exide to be all all conduction of the sublence of the world for doight the suphist of all are as the susself also a saiged and as a propensed the sulfurion of a plist of hind belong the light in streng his the stinct of here as the strange viriou\n",
      "\n",
      "...Diversity: 1.0\n",
      "...Generating with seed: \"em or world-pain in that which he finds \"\n",
      "...Generated:  able sun some and the himself whroken of his is mostle conduct rasadian himpal, as ithald many, abologr's that compsion morf crenble if the such plocoull'c his to him yot aboo deer not houd and disistitiof, areanby abore anstonive bean then \"live there inquered there good of emerual him of sursestowinaty in world; it, and now thes eostirations, froth rist-niffere to inverm; the vyaigs indess of in\n",
      "\n",
      "...Diversity: 1.2\n",
      "...Generating with seed: \"em or world-pain in that which he finds \"\n",
      "...Generated:  susprusiousays as loved the but ow portomy deterved). and nociagadit diffels). the agkin of wind hemply crabulads and dequite; and inguluy granopoly sleeq... \"helonges!  prowricting dimen; disextain. in; i mani--hausely newes. they greaturous to the angarm of sire\" poy fow u1recraoned we have dobecrsminifialinged his ginalsof appabens. sach ole to;--seffloss ays in a wowounmers of to ourte and ade\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: %d\\n\" % epoch)\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "\n",
    "print()\n",
    "print(\"Generating text after epoch: %d\" % epoch)\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print(\"...Diversity:\", diversity)\n",
    "\n",
    "    generated = \"\"\n",
    "    sentence = text[start_index : start_index + maxlen]\n",
    "    print('...Generating with seed: \"' + sentence + '\"')\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.0\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "\n",
    "    print(\"...Generated: \", generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.6572\n",
      "Epoch: 1\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.6116\n",
      "Epoch: 2\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.5735\n",
      "Epoch: 3\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.5413\n",
      "Epoch: 4\n",
      "\n",
      "3130/3130 [==============================] - 16s 5ms/step - loss: 1.5140\n",
      "Epoch: 5\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.4903\n",
      "Epoch: 6\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.4684\n",
      "Epoch: 7\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.4476\n",
      "Epoch: 8\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.4300\n",
      "Epoch: 9\n",
      "\n",
      "3130/3130 [==============================] - 15s 5ms/step - loss: 1.4148\n",
      "\n",
      "Generating text after epoch: 9\n",
      "...Diversity: 0.2\n",
      "...Generating with seed: \"t! a statesman who should do all this, w\"\n",
      "...Generated:  ith the strength of the same the destined his sense of the spirit of the strength of the same the strength of the strength, and are also a sort of such a strength of the strength of the same the incrantent in the present in the superstands and as the value of the spirit of the superstands and strength of the same the strength, in the special simple and all the sense of the conscience of the same t\n",
      "\n",
      "...Diversity: 0.5\n",
      "...Generating with seed: \"t! a statesman who should do all this, w\"\n",
      "...Generated:  ith the sendes and struck be a madical young the expense of one more proporacion and his one of it.                               wild so man overy the string in one mas believe and and all the same concealist of the one may as a find of all the famity as it is morality of the religional men as a sereation of his oper of men in the sense of such a standands, we far an ending for the word it is an \n",
      "\n",
      "...Diversity: 1.0\n",
      "...Generating with seed: \"t! a statesman who should do all this, w\"\n",
      "...Generated:  ith at propers: or neas and escurady the greatess from this cussed of inturalisy; on the plenup to be sank, one contience.=--that is an end. everything of poimpleped mase nenderwands to be deamiwing a many head nature:--de, at them give differeration in you--been grimblane can themrelpose fince in morality insurgan, say, we so was losk follow--(and courded of aniarour, dridifulfus, with they is lo\n",
      "\n",
      "...Diversity: 1.2\n",
      "...Generating with seed: \"t! a statesman who should do all this, w\"\n",
      "...Generated:  orther, no realont, to vigtation, the instivil naufle yope more othering of han mask oo his operance valies: 6wrshepentive to \"from his dencided antist even everyticusg nature chargms!: so rectives. in o, gevertses the moct douborsal gox in the immoral andmore as ones's have obradentitarly) in a with not much? which reloghesing awavian concerndardly we the alwaid end only trads doy hitherto anropi\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('deeplearning': conda)"
  },
  "interpreter": {
   "hash": "eddcb13c0afbe40eff33f8afdc713c878396e4478ab766ce2943b14c50b9ae00"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}